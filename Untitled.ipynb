{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e96c35d-edbd-4f41-9189-72d7a6973005",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Spark installtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1e1fb8-57ef-4851-a559-8830ef8813f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\kinja\\appdata\\roaming\\python\\python311\\site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\kinja\\appdata\\roaming\\python\\python311\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "503737e4-39c7-44c7-86d7-ce572705c940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\kinja\\appdata\\roaming\\python\\python311\\site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b81a021-5881-4f12-9856-5d26579635d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 3.4.1\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: C:\\Users\\kinja\\AppData\\Roaming\\Python\\Python311\\site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4fc35a8-58b5-46e1-981e-c40127959aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4057ae3c-a9ca-46fe-bd12-430e027018b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Data').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64f2897b-caf2-4a94-be03-6b404cfb5b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x19e78eae610>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be119d5e-ffc8-4591-bb11-c48c95c435f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+\n",
      "| ID|  Name|Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|  1| Sagar|            CSE|  UP| 10000|\n",
      "|  2|Shivam|             IT|  MP| 15000|\n",
      "|  3|  Muni|           Mech|  AP| 25000|\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the dataset\n",
    "spark.read.option('header','true').csv('D:/Hiral/Study/PySpark/test1.csv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f52cb8d9-a9d9-4764-9c24-222ff69dc3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+\n",
      "| ID|  Name|Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|  1| Sagar|            CSE|  UP| 10000|\n",
      "|  2|Shivam|             IT|  MP| 15000|\n",
      "|  3|  Muni|           Mech|  AP| 25000|\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('header','true').csv('D:/Hiral/Study/PySpark/test1.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11db3cbe-199b-40e9-b1fe-fda092e7547b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43c50e37-1cf7-4e05-93b1-99d19da712ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department_Name: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fccc81d-791c-4fed-b71d-296b9725f991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID', 'Name', 'Department_Name', 'City', 'Salary']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the column name \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af6c5460-6721-43cf-9aae-a8d91c209100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID='1', Name='Sagar', Department_Name='CSE', City='UP', Salary='10000'),\n",
       " Row(ID='2', Name='Shivam', Department_Name='IT', City='MP', Salary='15000')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dispaly first 2 records\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a275ccb-7801-435d-9b8b-27af26d4ab66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  Name|\n",
      "+------+\n",
      "| Sagar|\n",
      "|Shivam|\n",
      "|  Muni|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select any particular filed \n",
    "df.select(\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a819fe3-c4c2-4050-9295-86571e22ff0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ID', 'string'),\n",
       " ('Name', 'string'),\n",
       " ('Department_Name', 'string'),\n",
       " ('City', 'string'),\n",
       " ('Salary', 'string')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display column data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6adbb9a3-0c6c-4f4a-8405-2073b9347e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+---------------+----+------------------+\n",
      "|summary| ID|  Name|Department_Name|City|            Salary|\n",
      "+-------+---+------+---------------+----+------------------+\n",
      "|  count|  3|     3|              3|   3|                 3|\n",
      "|   mean|2.0|  null|           null|null|16666.666666666668|\n",
      "| stddev|1.0|  null|           null|null| 7637.626158259733|\n",
      "|    min|  1|  Muni|            CSE|  AP|             10000|\n",
      "|    max|  3|Shivam|           Mech|  UP|             25000|\n",
      "+-------+---+------+---------------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36752d47-f423-49c4-8e70-2f7d047dd9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+----+------+\n",
      "| ID|Department_Name|City|Salary|\n",
      "+---+---------------+----+------+\n",
      "|  1|            CSE|  UP| 10000|\n",
      "|  2|             IT|  MP| 15000|\n",
      "|  3|           Mech|  AP| 25000|\n",
      "+---+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop the any filed \n",
    "df.drop(\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6112ea46-24bd-4688-8c67-5086a76a4ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+\n",
      "| ID|  Name|Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|  1| Sagar|            CSE|  UP| 10000|\n",
      "|  2|Shivam|             IT|  MP| 15000|\n",
      "|  3|  Muni|           Mech|  AP| 25000|\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# recreate drop filed \n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81526122-d889-43a4-9700-0372b6e39ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+----+------+\n",
      "| ID|  Name|dept|City|Salary|\n",
      "+---+------+----+----+------+\n",
      "|  1| Sagar| CSE|  UP| 10000|\n",
      "|  2|Shivam|  IT|  MP| 15000|\n",
      "|  3|  Muni|Mech|  AP| 25000|\n",
      "+---+------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename any one column\n",
    "df.withColumnRenamed(\"Department_Name\" ,\"dept\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f42a14f0-dedc-41ea-96fe-6aacf73f6feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+----+------+\n",
      "| ID| fname|dept|City|Salary|\n",
      "+---+------+----+----+------+\n",
      "|  1| Sagar| CSE|  UP| 10000|\n",
      "|  2|Shivam|  IT|  MP| 15000|\n",
      "|  3|  Muni|Mech|  AP| 25000|\n",
      "+---+------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# multiple column rename \n",
    "df.withColumnRenamed(\"Department_Name\",\"dept\").withColumnRenamed(\"Name\",\"fname\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "657ffc54-ab21-4e3e-9e38-3ed6c06b118a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+\n",
      "| ID|  Name|Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|  1| Sagar|            CSE|  UP| 10000|\n",
      "|  2|Shivam|             IT|  MP| 15000|\n",
      "|  3|  Muni|           Mech|  AP| 25000|\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06b2741c-bd28-49fd-a8ed-9de40d56ee28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+\n",
      "| ID|  Name|Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|  1| Sagar|            CSE|  UP| 10000|\n",
      "|  2|Shivam|             IT|  MP| 15000|\n",
      "|  3|  Muni|           Mech|  AP| 25000|\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='all',thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91ec9c2b-70b3-4238-996a-f03d4ae81744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+\n",
      "| ID|  Name|Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|  1| Sagar|            CSE|  UP| 10000|\n",
      "|  2|Shivam|             IT|  MP| 15000|\n",
      "|  3|  Muni|           Mech|  AP| 25000|\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='all',subset=['Salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e7080f6-5267-4185-9417-e5d6b9b6403c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+\n",
      "| ID|  Name|Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|  1| Sagar|            CSE|  UP| 10000|\n",
      "|  2|Shivam|             IT|  MP| 15000|\n",
      "|  3|  Muni|           Mech|  AP| 25000|\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('Missing Value',['Salary']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b44d3a-f6d1-474a-b3f1-cb980d2a6de0",
   "metadata": {},
   "source": [
    "## Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ae77e03-80cc-46eb-9646-1c6b8ba671ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+\n",
      "| ID|  Name|Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|  1| Sagar|            CSE|  UP| 10000|\n",
      "|  2|Shivam|             IT|  MP| 15000|\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('Salary<=15000').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "481af873-1440-4850-935a-d06f3181dc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|  Name|Department_Name|\n",
      "+------+---------------+\n",
      "| Sagar|            CSE|\n",
      "|Shivam|             IT|\n",
      "+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('Salary<=15000').select(['Name','Department_Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31af7beb-55c9-41be-b5e9-38ca168dfd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+\n",
      "| ID|  Name|Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|  1| Sagar|            CSE|  UP| 10000|\n",
      "|  2|Shivam|             IT|  MP| 15000|\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Salary']<=20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7a573e5-83d0-43c7-97c4-83c3fae40a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Salary|\n",
      "+------+\n",
      "| 10000|\n",
      "| 25000|\n",
      "| 15000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Salary').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0411aed-b66c-41dd-8ea1-6fba83ab133e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Salary|\n",
      "+------+\n",
      "| 10000|\n",
      "| 25000|\n",
      "| 15000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Salary').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bdfa2de-6242-4ec0-b644-12c98fe7e07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Salary|\n",
      "+------+\n",
      "| 10000|\n",
      "| 25000|\n",
      "| 15000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Salary').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "856d996a-9231-4fa8-b703-47a900a69e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Salary|count|\n",
      "+------+-----+\n",
      "| 10000|    1|\n",
      "| 25000|    1|\n",
      "| 15000|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Salary').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91b21e04-825a-4012-a720-341b21717c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|    50000.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f494f0f-bf73-4ddb-8915-5101fdf77db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|Department_Name|\n",
      "+---------------+\n",
      "|             IT|\n",
      "|           Mech|\n",
      "|            CSE|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Department_Name').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec7096ee-2b40-4392-8c80-5059677ba740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|Department_Name|\n",
      "+---------------+\n",
      "|             IT|\n",
      "|           Mech|\n",
      "|            CSE|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Department_Name').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c5fcba3-b748-49f9-983a-18aa9f6d8acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  Name|\n",
      "+------+\n",
      "|Shivam|\n",
      "|  Muni|\n",
      "| Sagar|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Name').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc5a116a-4277-491c-9e70-c2369c4b9d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+\n",
      "| ID|  Name|Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|  3|  Muni|           Mech|  AP| 25000|\n",
      "|  1| Sagar|            CSE|  UP| 10000|\n",
      "|  2|Shivam|             IT|  MP| 15000|\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "800ca261-7570-4838-8868-b444497c4144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+\n",
      "| ID|  Name|Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|  3|  Muni|           Mech|  AP| 25000|\n",
      "|  2|Shivam|             IT|  MP| 15000|\n",
      "|  1| Sagar|            CSE|  UP| 10000|\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"Department_Name\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f91d620-7ec9-43c5-b535-d27bece9726d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+\n",
      "| ID|  Name|Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|  3|  Muni|           Mech|  AP| 25000|\n",
      "|  2|Shivam|             IT|  MP| 15000|\n",
      "|  1| Sagar|            CSE|  UP| 10000|\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.Department_Name.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2dbca57e-8944-41f1-b46e-dcbb8335e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+\n",
      "| ID|  Name|Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|  2|Shivam|             IT|  MP| 15000|\n",
      "|  1| Sagar|            CSE|  UP| 10000|\n",
      "|  3|  Muni|           Mech|  AP| 25000|\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.Name.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "799dc429-b388-4fbb-aec6-eb5db4a5c3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+\n",
      "| ID|  Name|Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|  3|  Muni|           Mech|  AP| 25000|\n",
      "|  1| Sagar|            CSE|  UP| 10000|\n",
      "|  2|Shivam|             IT|  MP| 15000|\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.Name,df.Department_Name.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2446f144-fa88-4f72-abb5-044a2b3287c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|  Name|Department_Name|\n",
      "+------+---------------+\n",
      "| Sagar|            CSE|\n",
      "|Shivam|             IT|\n",
      "|  Muni|           Mech|\n",
      "+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Name\",\"Department_Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e86aa6f4-7f6e-4013-9a66-d5d6027dc99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+----------+\n",
      "| ID|  Name|Department_Name|City|Salary|New Salary|\n",
      "+---+------+---------------+----+------+----------+\n",
      "|  1| Sagar|            CSE|  UP| 10000|   12000.0|\n",
      "|  2|Shivam|             IT|  MP| 15000|   17000.0|\n",
      "|  3|  Muni|           Mech|  AP| 25000|   27000.0|\n",
      "+---+------+---------------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('New Salary',df['Salary']+2000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06a09a83-3ca8-4e81-b063-47a4b2f93047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+\n",
      "| ID|  Name|Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|  1| Sagar|            CSE|  UP| 10000|\n",
      "|  2|Shivam|             IT|  MP| 15000|\n",
      "|  3|  Muni|           Mech|  AP| 25000|\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop('New Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15a74e34-9f85-4a77-8fe1-ce2f53990446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count: 3\n",
      "+---+------+---------------+----+------+\n",
      "|ID |Name  |Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|3  |Muni  |Mech           |AP  |25000 |\n",
      "|2  |Shivam|IT             |MP  |15000 |\n",
      "|1  |Sagar |CSE            |UP  |10000 |\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropDuplicates() function which returns a new DataFrame after removing duplicate rows.\n",
    "df = df.dropDuplicates()\n",
    "print(\"Distinct count: \"+str(df.count()))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39b4e875-4f99-441f-b873-5ba9cfeda791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count: 3\n",
      "+---+------+---------------+----+------+\n",
      "|ID |Name  |Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|3  |Muni  |Mech           |AP  |25000 |\n",
      "|2  |Shivam|IT             |MP  |15000 |\n",
      "|1  |Sagar |CSE            |UP  |10000 |\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Applying distinct() to remove duplicate rows\n",
    "df = df.distinct()\n",
    "print(\"Distinct count: \"+str(df.count()))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b0553d08-c28d-43bc-9e52-737f7f8b4a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|first(salary)|\n",
      "+-------------+\n",
      "|10000        |\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first() function returns the first element in a column when ignoreNulls is set to true, it returns the first non-null element.\n",
    "from pyspark.sql.functions import first\n",
    "df.select(first(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2480ccda-b109-4701-a5bf-c6684169d40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|last(Salary)|\n",
      "+------------+\n",
      "|25000       |\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# last() function returns the last element in a column. when ignoreNulls is set to true, it returns the last non-null element.\n",
    "from pyspark.sql.functions import last\n",
    "df.select(last(\"Salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4e3cb37-167e-43bc-9560-2c8155159647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approx_count_distinct: 3\n"
     ]
    }
   ],
   "source": [
    "# approx_count_distinct() function returns the count of distinct items in a group.\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "print(\"approx_count_distinct: \" + \\\n",
    "      str(df.select(approx_count_distinct(\"Department_Name\")).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "acabb41f-3f04-477e-a1d5-d175e316e0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg: 16666.666666666668\n"
     ]
    }
   ],
   "source": [
    "# avg() function returns the average of values in the input column.\n",
    "from pyspark.sql.functions import avg\n",
    "print(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8fa8aae3-f1fc-44b3-81fc-ae6f48b25047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|collect_set(Department_Name)|\n",
      "+----------------------------+\n",
      "|[Mech, IT, CSE]             |\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# collect_set() function returns all values from an input column with duplicate values eliminated.\n",
    "from pyspark.sql.functions import collect_set\n",
    "df.select(collect_set(\"Department_Name\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ed7038e-78b3-4a8d-95a3-94f9b83f5658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: Row(count(Department_Name)=3)\n"
     ]
    }
   ],
   "source": [
    "# count() function returns number of elements in a column.\n",
    "from pyspark.sql.functions import count\n",
    "print(\"count: \"+str(df.select(count(\"Department_Name\")).collect()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67ed8692-abc7-4c83-8f58-f94808c1dbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|kurtosis(Salary)   |\n",
      "+-------------------+\n",
      "|-1.4999999999999998|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kurtosis() function returns the kurtosis of the values in a group.\n",
    "from pyspark.sql.functions import kurtosis\n",
    "df.select(kurtosis(\"Salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c29fec7-3302-45ac-92ca-8c70abd5ff2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|max(Salary)|\n",
      "+-----------+\n",
      "|25000      |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# max() function returns the maximum value in a column.\n",
    "from pyspark.sql.functions import max\n",
    "df.select(max(\"Salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8d40e29-585b-40c0-91f5-02fc164b94fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(Salary)|\n",
      "+-----------+\n",
      "|      10000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# min() function returns the minimum value in a column.\n",
    "from pyspark.sql.functions import min\n",
    "df.select(min(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0daa5911-7f2f-4c73-bac0-ae96e34cb8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       avg(Salary)|\n",
      "+------------------+\n",
      "|16666.666666666668|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mean() function returns the average of the values in a column. Alias for Avg\n",
    "from pyspark.sql.functions import mean \n",
    "df.select(mean(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "605eb77b-bd70-4f27-8e40-ce177823971d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|   skewness(Salary)|\n",
      "+-------------------+\n",
      "|0.38180177416060607|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# skewness() function returns the skewness of the values in a group.\n",
    "from pyspark.sql.functions import skewness\n",
    "df.select(skewness(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab9e13fe-b944-44eb-831d-45efff572704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+\n",
      "|stddev_samp(Salary)|stddev_samp(Salary)|stddev_pop(Salary)|\n",
      "+-------------------+-------------------+------------------+\n",
      "|7637.626158259733  |7637.626158259733  |6236.095644623236 |\n",
      "+-------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stddev() alias for stddev_samp.\n",
    "# stddev_samp() function returns the sample standard deviation of values in a column.\n",
    "# stddev_pop() function returns the population standard deviation of the values in a column.\n",
    "\n",
    "from pyspark.sql.functions import stddev,stddev_samp,stddev_pop\n",
    "df.select(stddev(\"Salary\"),stddev_samp(\"Salary\"),stddev_pop(\"Salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4179dba0-f504-4d97-a2a3-346855e31735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|    50000.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sum() function Returns the sum of all values in a column.\n",
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab406355-e8f9-4e79-a4a0-fe93f5d726ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kinja\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\sql\\functions.py:752: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|sum(DISTINCT Salary)|\n",
      "+--------------------+\n",
      "|             50000.0|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sumDistinct() function returns the sum of all distinct values in a column.\n",
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12f098a8-8f41-4f76-bc2c-aa12c6b9f291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|   var_samp(Salary)|   var_samp(Salary)|    var_pop(Salary)|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|5.833333333333333E7|5.833333333333333E7|3.888888888888889E7|\n",
      "+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# variance() alias for var_samp\n",
    "# var_samp() function returns the unbiased variance of the values in a column.\n",
    "# var_pop() function returns the population variance of the values in a column.\n",
    "from pyspark.sql.functions import variance,var_samp,var_pop\n",
    "df.select(variance(\"Salary\"),var_samp(\"Salary\"),var_pop(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4159e2d-8a49-4666-b5f0-666b80bc0f96",
   "metadata": {},
   "source": [
    "## Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9500801-75f7-456c-936e-d0c6ebac093e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+----------+\n",
      "|ID |Name  |Department_Name|City|Salary|row_number|\n",
      "+---+------+---------------+----+------+----------+\n",
      "|1  |Sagar |CSE            |UP  |10000 |1         |\n",
      "|2  |Shivam|IT             |MP  |15000 |1         |\n",
      "|3  |Muni  |Mech           |AP  |25000 |1         |\n",
      "+---+------+---------------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# row_number() window function gives the sequential row number starting from 1 to the result of each window partition.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "windowSpec = Window.partitionBy(\"Department_Name\").orderBy(\"Salary\")\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "757af33a-3ec0-49e9-9dc5-f04ece73af55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+----+\n",
      "| ID|  Name|Department_Name|City|Salary|rank|\n",
      "+---+------+---------------+----+------+----+\n",
      "|  1| Sagar|            CSE|  UP| 10000|   1|\n",
      "|  2|Shivam|             IT|  MP| 15000|   1|\n",
      "|  3|  Muni|           Mech|  AP| 25000|   1|\n",
      "+---+------+---------------+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rank() window function provides a rank to the result within a window partition. This function leaves gaps in rank when there are ties.\n",
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "665a3814-f2eb-4793-bb8e-2b6268a105a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+----------+\n",
      "| ID|  Name|Department_Name|City|Salary|dense_rank|\n",
      "+---+------+---------------+----+------+----------+\n",
      "|  1| Sagar|            CSE|  UP| 10000|         1|\n",
      "|  2|Shivam|             IT|  MP| 15000|         1|\n",
      "|  3|  Muni|           Mech|  AP| 25000|         1|\n",
      "+---+------+---------------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dense_rank() window function is used to get the result with rank of rows within a window partition without any gaps.\n",
    "from pyspark.sql.functions import dense_rank \n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d12b2b9-d48b-4b4f-b721-1d9179a7314b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+------------+\n",
      "| ID|  Name|Department_Name|City|Salary|percent_rank|\n",
      "+---+------+---------------+----+------+------------+\n",
      "|  1| Sagar|            CSE|  UP| 10000|         0.0|\n",
      "|  2|Shivam|             IT|  MP| 15000|         0.0|\n",
      "|  3|  Muni|           Mech|  AP| 25000|         0.0|\n",
      "+---+------+---------------+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# percent_rank()\n",
    "from pyspark.sql.functions import percent_rank\n",
    "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0961c6b8-913b-46f1-b7cd-f817d70ad32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+-----+\n",
      "| ID|  Name|Department_Name|City|Salary|ntile|\n",
      "+---+------+---------------+----+------+-----+\n",
      "|  1| Sagar|            CSE|  UP| 10000|    1|\n",
      "|  2|Shivam|             IT|  MP| 15000|    1|\n",
      "|  3|  Muni|           Mech|  AP| 25000|    1|\n",
      "+---+------+---------------+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ntile() window function returns the relative rank of result rows within a window partition.\n",
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\",ntile(2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0e0fe03e-5310-4542-8aaa-113cc3958af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+---------+\n",
      "| ID|  Name|Department_Name|City|Salary|cume_dist|\n",
      "+---+------+---------------+----+------+---------+\n",
      "|  1| Sagar|            CSE|  UP| 10000|      1.0|\n",
      "|  2|Shivam|             IT|  MP| 15000|      1.0|\n",
      "|  3|  Muni|           Mech|  AP| 25000|      1.0|\n",
      "+---+------+---------------+----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cume_dist() This function computes the cumulative distribution of a value within a window partition.\n",
    "from pyspark.sql.functions import cume_dist\n",
    "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "38306163-687a-4a4e-889c-5dab63772bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+----+\n",
      "| ID|  Name|Department_Name|City|Salary| lag|\n",
      "+---+------+---------------+----+------+----+\n",
      "|  1| Sagar|            CSE|  UP| 10000|null|\n",
      "|  2|Shivam|             IT|  MP| 15000|null|\n",
      "|  3|  Muni|           Mech|  AP| 25000|null|\n",
      "+---+------+---------------+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lag() function allows you to access a previous row’s value within the partition based on a specified offset. \n",
    "from pyspark.sql.functions import lag\n",
    "df.withColumn(\"lag\",lag(\"Salary\",2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e2061dff-7d09-4703-baee-8ae11b528a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+----+\n",
      "| ID|  Name|Department_Name|City|Salary|lead|\n",
      "+---+------+---------------+----+------+----+\n",
      "|  1| Sagar|            CSE|  UP| 10000|null|\n",
      "|  2|Shivam|             IT|  MP| 15000|null|\n",
      "|  3|  Muni|           Mech|  AP| 25000|null|\n",
      "+---+------+---------------+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lead() function retrieves the column value from the following row within the partition based on a specified offset. \n",
    "from pyspark.sql.functions import lead    \n",
    "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4529b45c-be16-4d1b-8395-bc17d9cccb6c",
   "metadata": {},
   "source": [
    "## Create sql table in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8ced6b3f-248f-45ce-8b5f-eb1f13d891e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+\n",
      "| ID|  Name|Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|  3|  Muni|           Mech|  AP| 25000|\n",
      "|  2|Shivam|             IT|  MP| 15000|\n",
      "|  1| Sagar|            CSE|  UP| 10000|\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create sql table \n",
    "df.createOrReplaceTempView(\"Emp\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4ab9a988-f32b-4e00-a105-e9466538f3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+------+\n",
      "|  Name|Department_Name|Salary|\n",
      "+------+---------------+------+\n",
      "|  Muni|           Mech| 25000|\n",
      "|Shivam|             IT| 15000|\n",
      "| Sagar|            CSE| 10000|\n",
      "+------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select column from the table \n",
    "df.select(\"Name\",\"Department_Name\",\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "55863743-6dcc-4508-84d7-77c0ffdd9e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+------+\n",
      "|  Name|Department_Name|Salary|\n",
      "+------+---------------+------+\n",
      "|Shivam|             IT| 15000|\n",
      "+------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql where clause\n",
    "df.select(\"Name\",\"Department_Name\",\"Salary\").where(\"Department_Name = 'IT'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "325b9ffe-47f6-4f03-a015-61b3d95d6000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+------+\n",
      "|  Name|Department_Name|Salary|\n",
      "+------+---------------+------+\n",
      "| Sagar|            CSE| 10000|\n",
      "|Shivam|             IT| 15000|\n",
      "|  Muni|           Mech| 25000|\n",
      "+------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sql order by\n",
    "df.select(\"Name\",\"Department_Name\",\"Salary\").where(\"Department_Name in ('CSE','IT','Mech')\").orderBy(\"Department_Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0497f949-1f24-46a8-b7fc-25fc7f62ebb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+\n",
      "| ID|  Name|Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|  3|  Muni|           Mech|  AP| 25000|\n",
      "|  2|Shivam|             IT|  MP| 15000|\n",
      "|  1| Sagar|            CSE|  UP| 10000|\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all the columns \n",
    "df.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ded0acf2-87c9-40d5-abf4-32792290bfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|Department_Name|count|\n",
      "+---------------+-----+\n",
      "|             IT|    1|\n",
      "|           Mech|    1|\n",
      "|            CSE|    1|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by \n",
    "df.groupby(\"Department_Name\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cdc4522a-1079-4de0-af24-865290be8081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|Department_Name|\n",
      "+---------------+\n",
      "|           Mech|\n",
      "|             IT|\n",
      "|            CSE|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alias \n",
    "from pyspark.sql.functions import when,col\n",
    "from pyspark.sql import functions as F\n",
    "df.select(F.col(\"Department_Name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ddc9e-fd5e-49f7-9c61-d5eb7165a15e",
   "metadata": {},
   "source": [
    "## PySpark SQL Date and Timestamp Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ddeb9-dcae-4a54-b338-599b79bd0de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6710bf00-7066-49fc-b5c2-c2e70e328a74",
   "metadata": {},
   "source": [
    "## PySpark SQL Types (DataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "21dc148a-801c-47f4-8faa-65ccaeb37ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'array', 'elementType': 'integer', 'containsNull': False}\n",
      "array<int>\n",
      "array\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "arrayType = ArrayType(IntegerType(),False)\n",
    "print(arrayType.jsonValue())\n",
    "print(arrayType.simpleString())\n",
    "print(arrayType.typeName())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b32df25b-7689-4b11-8271-84285bd7fff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "IntegerType()\n"
     ]
    }
   ],
   "source": [
    "# Use ArrayType to represent arrays in a DataFrame and use ArrayType() to get an array object of a specific type.\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "arrayType = ArrayType(IntegerType(),False)\n",
    "print(arrayType.containsNull)\n",
    "print(arrayType.elementType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3a11a82b-ee10-44e1-a525-28b4ea1f34da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringType()\n",
      "IntegerType()\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Use MapType to represent key-value pair in a DataFrame. Use MapType() to get a map object of a specific key and value type.\n",
    "from pyspark.sql.types import MapType,StringType,IntegerType\n",
    "mapType = MapType(StringType(),IntegerType())\n",
    " \n",
    "print(mapType.keyType)\n",
    "print(mapType.valueType)\n",
    "print(mapType.valueContainsNull)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc24c149-6d48-4faf-8fd4-3ae8b780743f",
   "metadata": {},
   "source": [
    "## How to create RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ce0c0d23-78df-4b59-bf15-ae882bb3a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0333d921-94ec-407b-8baa-3f18f6df1074",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdds = spark.sparkContext.parallelize([('Mumbai',1),('Pune',2),('Surat',3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4ec3593b-e232-4c1b-8223-34d1bb29e663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[485] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c4f5fa16-33b2-4d62-950d-c803a3363da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mumbai', 1), ('Pune', 2), ('Surat', 3)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5da855de-ed58-420a-ba95-a193ed81b714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From local[5] : 12\n"
     ]
    }
   ],
   "source": [
    "# Create spark session with local[5]\n",
    "rdd = spark.sparkContext.parallelize(range(0,20))\n",
    "print(\"From local[5] : \"+str(rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3f0902ec-57af-4988-b749-45b92df211c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallelize : 6\n",
      "TextFile : 11\n"
     ]
    }
   ],
   "source": [
    "# Use parallelize with 6 partitions\n",
    "rdd1 = spark.sparkContext.parallelize(range(0,25), 6)\n",
    "print(\"parallelize : \"+str(rdd1.getNumPartitions()))\n",
    "\n",
    "rddFromFile = spark.sparkContext.textFile(\"D:/Hiral/Study/PySpark/test1.csv\",10)\n",
    "print(\"TextFile : \"+str(rddFromFile.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4eedf9bc-adf2-46c6-ab5a-3848402d6f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# DataFrame coalesce() is used only to decrease the number of partitions.\n",
    "df3 = df.coalesce(2)\n",
    "print(df3.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b8c8f8de-3985-4f59-8afc-3fa2fedff46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# the PySpark DataFrame repartition() method is used to increase or decrease the partitions.\n",
    "df2 = df.repartition(6)\n",
    "print(df2.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8f92d281-f892-4017-8fe5-287a80e5c877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmptyRDD[512] at emptyRDD at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "#Creates Empty RDD\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "print(emptyRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e3a58edd-cdf3-4dec-bcf7-ce11b143f173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[513] at readRDDFromFile at PythonRDD.scala:274\n"
     ]
    }
   ],
   "source": [
    "#Creates Empty RDD using parallelize\n",
    "rdd = spark.sparkContext.parallelize([])\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aaeb4184-e30f-4db2-a49a-500a07652e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('firstname', StringType(), True), StructField('middlename', StringType(), True), StructField('lastname', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "#Create Schema\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "schema = StructType([\n",
    "  StructField('firstname', StringType(), True),\n",
    "  StructField('middlename', StringType(), True),\n",
    "  StructField('lastname', StringType(), True)\n",
    "  ])\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0e7dfb73-7324-47d4-a7d6-6a0f6fc286c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Name' exists in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Check if a column exists\n",
    "if \"Name\" in df.columns:\n",
    "    print(\"Column 'Name' exists in the DataFrame.\")\n",
    "else:\n",
    "    print(\"Column 'Name' does not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e2af31b8-c674-4f9d-91fc-3048c1256bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df2 = df.withColumn(\"City\", when(df.City == \"M\",\"Male\")\n",
    "                                 .when(df.City == \"F\",\"Female\")\n",
    "                                 .when(df.City.isNull() ,\"\")\n",
    "                                 .otherwise(df.City))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "978b017d-c606-4c2b-a2ad-ccbcede6d9fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 18\u001b[0m\n\u001b[0;32m     11\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([ \\\n\u001b[0;32m     12\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m,StringType(),\u001b[38;5;28;01mTrue\u001b[39;00m), \\\n\u001b[0;32m     13\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m) \\\n\u001b[0;32m     14\u001b[0m   ])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Create a DataFrame from the data\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(data\u001b[38;5;241m=\u001b[39mdata, schema\u001b[38;5;241m=\u001b[39mschema)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Show the DataFrame\u001b[39;00m\n\u001b[0;32m     21\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\sql\\session.py:1276\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1272\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m   1275\u001b[0m     )\n\u001b[1;32m-> 1276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[0;32m   1277\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1278\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\sql\\session.py:1320\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1318\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n\u001b[0;32m   1321\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[38;5;241m.\u001b[39mrdd(), struct\u001b[38;5;241m.\u001b[39mjson())\n\u001b[0;32m   1322\u001b[0m df \u001b[38;5;241m=\u001b[39m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\rdd.py:4897\u001b[0m, in \u001b[0;36mRDD._to_java_object_rdd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4894\u001b[0m rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pickled()\n\u001b[0;32m   4895\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4897\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mpythonToJava(rdd\u001b[38;5;241m.\u001b[39m_jrdd, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\rdd.py:5441\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5439\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 5441\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m _wrap_function(\n\u001b[0;32m   5442\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd_deserializer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer, profiler\n\u001b[0;32m   5443\u001b[0m )\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\n\u001b[0;32m   5447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier\n\u001b[0;32m   5448\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\rdd.py:5243\u001b[0m, in \u001b[0;36m_wrap_function\u001b[1;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[0;32m   5241\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[0;32m   5242\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 5243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSimplePythonFunction(\n\u001b[0;32m   5244\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[0;32m   5245\u001b[0m     env,\n\u001b[0;32m   5246\u001b[0m     includes,\n\u001b[0;32m   5247\u001b[0m     sc\u001b[38;5;241m.\u001b[39mpythonExec,\n\u001b[0;32m   5248\u001b[0m     sc\u001b[38;5;241m.\u001b[39mpythonVer,\n\u001b[0;32m   5249\u001b[0m     broadcast_vars,\n\u001b[0;32m   5250\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[0;32m   5251\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define the data as a list of tuples\n",
    "data = [(\"John\", 25), (\"Alice\", 30), (\"Bob\", 35)]\n",
    "print(type(data))\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"Name\",StringType(),True), \\\n",
    "    StructField(\"Age\", IntegerType(), True) \\\n",
    "  ])\n",
    "\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7676046-3371-4a6a-9d7b-5d85b37c0fdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local) created by __init__ at C:\\Users\\kinja\\AppData\\Local\\Temp\\ipykernel_20800\\4106845886.py:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sc \u001b[38;5;241m=\u001b[39m pyspark\u001b[38;5;241m.\u001b[39mSparkContext(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJohn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m25\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m30\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBob\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m35\u001b[39m)]\n\u001b[0;32m      4\u001b[0m pd\u001b[38;5;241m.\u001b[39mdataframe(data)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m     )\n\u001b[1;32m--> 198\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    201\u001b[0m         master,\n\u001b[0;32m    202\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    213\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\context.py:445\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    442\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[1;32m--> 445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    450\u001b[0m             currentAppName,\n\u001b[0;32m    451\u001b[0m             currentMaster,\n\u001b[0;32m    452\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[0;32m    453\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[0;32m    454\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[0;32m    455\u001b[0m         )\n\u001b[0;32m    456\u001b[0m     )\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local) created by __init__ at C:\\Users\\kinja\\AppData\\Local\\Temp\\ipykernel_20800\\4106845886.py:2 "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5be95786-18eb-4937-b334-309347951ba6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      2\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m----> 3\u001b[0m df\u001b[38;5;241m=\u001b[39md\u001b[38;5;241m.\u001b[39mtoDF()\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\sql\\session.py:115\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    +---+\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sparkSession\u001b[38;5;241m.\u001b[39mcreateDataFrame(\u001b[38;5;28mself\u001b[39m, schema, sampleRatio)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\sql\\session.py:1276\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1272\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m   1275\u001b[0m     )\n\u001b[1;32m-> 1276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[0;32m   1277\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1278\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\sql\\session.py:1316\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1313\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[1;32m-> 1316\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1318\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\sql\\session.py:931\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 931\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchema(rdd, samplingRatio, names\u001b[38;5;241m=\u001b[39mschema)\n\u001b[0;32m    932\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[0;32m    933\u001b[0m     tupled_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\sql\\session.py:874\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[1;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inferSchema\u001b[39m(\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    855\u001b[0m     rdd: RDD[Any],\n\u001b[0;32m    856\u001b[0m     samplingRatio: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    857\u001b[0m     names: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StructType:\n\u001b[0;32m    859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;124;03m    Infer schema from an RDD of Row, dict, or tuple.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;124;03m    :class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 874\u001b[0m     first \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mfirst()\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(first) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first row in RDD is empty, can not infer schema\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\rdd.py:2869\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2843\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   2844\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2845\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   2846\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2867\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   2868\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2869\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   2870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[0;32m   2871\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\rdd.py:2836\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   2833\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2835\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 2836\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39mrunJob(\u001b[38;5;28mself\u001b[39m, takeUpToNumLeft, p)\n\u001b[0;32m   2838\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m   2839\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\context.py:2319\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   2317\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   2318\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2319\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mrunJob(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd, partitions)\n\u001b[0;32m   2320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\rdd.py:5441\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5439\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 5441\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m _wrap_function(\n\u001b[0;32m   5442\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd_deserializer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer, profiler\n\u001b[0;32m   5443\u001b[0m )\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\n\u001b[0;32m   5447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier\n\u001b[0;32m   5448\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\rdd.py:5243\u001b[0m, in \u001b[0;36m_wrap_function\u001b[1;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[0;32m   5241\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[0;32m   5242\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 5243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSimplePythonFunction(\n\u001b[0;32m   5244\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[0;32m   5245\u001b[0m     env,\n\u001b[0;32m   5246\u001b[0m     includes,\n\u001b[0;32m   5247\u001b[0m     sc\u001b[38;5;241m.\u001b[39mpythonExec,\n\u001b[0;32m   5248\u001b[0m     sc\u001b[38;5;241m.\u001b[39mpythonVer,\n\u001b[0;32m   5249\u001b[0m     broadcast_vars,\n\u001b[0;32m   5250\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[0;32m   5251\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
