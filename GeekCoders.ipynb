{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ba224a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\kinja\\appdata\\roaming\\python\\python311\\site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\kinja\\appdata\\roaming\\python\\python311\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a2c7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\kinja\\appdata\\roaming\\python\\python311\\site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9277d87d-68e1-4fcb-970c-c0cc2ef5dac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 3.4.1\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: C:\\Users\\kinja\\AppData\\Roaming\\Python\\Python311\\site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b616f1fb-3b34-4ef2-9baf-8fa342c50b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c55be208-a867-43ad-a598-0dc24b1d4eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c984357c-e6ff-4d6c-a92c-5d4e5950983a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Dataframe</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x19e464bb6d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e55ec2f-8e68-49ac-a910-ca8a90c078f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+----+------+\n",
      "| ID|  Name|Department_Name|City|Salary|\n",
      "+---+------+---------------+----+------+\n",
      "|  1| Sagar|            CSE|  UP| 10000|\n",
      "|  2|Shivam|             IT|  MP| 15000|\n",
      "|  3|  Muni|           Mech|  AP| 25000|\n",
      "+---+------+---------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('header','true').csv('D:/Hiral/Study/PySpark/test1.csv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c536c09-a1bb-43fc-bf9b-a4c66881f9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef44d2-ae88-44c6-bac4-a14c4aecc1bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f15049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490411b-aa40-42a6-a3f5-b9ea7f5efeb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667f1f78-e974-430c-ac50-d00b98307032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ad7f1d8-83bb-40ae-851c-3d07dc7a2b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d14a8-45df-4e41-b96b-8933e9eff45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acaa1d3-da4b-453b-aba5-3c463c7ba508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f83e7f-5a20-45e3-a631-1b6748ede9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4651ed2f-5dad-4d27-90b6-e0968e07d431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b392db-32ec-4dd9-8e7b-c59b4b55ca8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39782cc5-f169-4f15-86c9-7a562bb2a052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0553dc-de25-4de9-abdd-42a0c915db62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04360c2a-ce0b-4447-964f-5487455e8f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'withColumnRenamed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDepartment_Name\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDept\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'withColumnRenamed'"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('Department_Name','Dept').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "872ca61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'withColumnRenamed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDepartment_Name\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDept\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSlry\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalary\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'withColumnRenamed'"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('Department_Name','Dept').withColumnRenamed('Slry','Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38fdb68-0234-40b8-aeb7-2106c0dcad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d13ece6-ee7a-47f3-897b-93ae3b9f029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c87d0-2753-4bfe-bf39-044cb2b2fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.drop(how='all',thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940117ac-68d0-4e9c-bf69-5e3805def324",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.drop(how='all',subset=['Salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa10a0a-1178-4cfd-a8d8-8448e37670d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill('Missing Value',['Salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b2dfa-6358-4908-afc1-c5deaabe25c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter('Salary<=20000').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a9bc2-d086-473d-b9f2-115c27a0c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter('Salary<=15000').select(['Name','Departments']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e26a24-51e2-47f3-a01f-c1bb3542c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df['Salary']<=20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e243adff-e6bd-4bd4-bf8b-7521ef8ecd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Salary').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c7f73-d6f8-4600-86c3-a3b6b2479ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207d067a-cc3f-4471-a4c9-32ef6bbf133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Departments').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e1d6e5-27a6-4a00-8f28-cc596fcb9f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Departments').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3499d80-24e0-4939-ad62-576880a51035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ee74aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Departments').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501768ae-4953-46fa-9d52-0a6fa6940cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Departments').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c8acfb-5fb3-4c80-8226-7a14f4308357",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Departments').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6eb615-3f19-41d6-b30b-12a9110f46af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy('Departments').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405eda17-ff39-4bb8-acf7-d02861469221",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(\"Departments\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf83421-52bb-4926-97db-8ebf7a25b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy(df.Departments.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9673532-11f4-4d7f-8b73-d94fd9d94218",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(df.Departments.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d982dbfb-d442-4469-b19d-178b2078606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(df.Name,df.Departments.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c8bb66-afdf-46e7-86ba-e158f5de856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Name\",\"Departments\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8911c-f7d2-4a21-b0cc-a98499326088",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('New Salary',df['Salary']+2000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b5a44-e778-45a0-a40f-e8ecb8205b70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop('New Salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddd7072-a5df-4223-9597-06032e0572a1",
   "metadata": {},
   "source": [
    "## How to create RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc4e50e-b86a-4e9a-baa5-96b49f12c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a72479a-054a-43bb-b681-5bfdde72cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdds = spark.sparkContext.parallelize([('Mumbai',1),('Pune',2),('Surat',3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc5185-1de5-4de7-80f4-c87d6f7335a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb1cf35-f2a3-437d-b609-b20cde0e4dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba74272-b1ee-47e1-b060-7cabf3f7d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session with local[5]\n",
    "rdd = spark.sparkContext.parallelize(range(0,20))\n",
    "print(\"From local[5] : \"+str(rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d7678-40c8-4fc7-abc9-98f574559450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use parallelize with 6 partitions\n",
    "rdd1 = spark.sparkContext.parallelize(range(0,25), 6)\n",
    "print(\"parallelize : \"+str(rdd1.getNumPartitions()))\n",
    "\n",
    "rddFromFile = spark.sparkContext.textFile(\"D:/Hiral/Study/PySpark/EmpDetails.csv\",10)\n",
    "print(\"TextFile : \"+str(rddFromFile.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d8ac2b-0a0c-408a-a1d0-7d2122420251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame coalesce() is used only to decrease the number of partitions.\n",
    "df3 = df.coalesce(2)\n",
    "print(df3.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44a9d22-dc58-4a61-ad2c-90163014bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the PySpark DataFrame repartition() method is used to increase or decrease the partitions.\n",
    "df2 = df.repartition(6)\n",
    "print(df2.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159cf5e6-d2dd-40d4-9e65-2a6eb096a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame example\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Dataframe.com') \\\n",
    "        .master(\"local[*]\").getOrCreate()\n",
    "\n",
    "df=spark.range(0,10)\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.show()\n",
    "df.toPandas().to_csv(\"test5.csv\")\n",
    "    #df.write.mode(\"overwrite\").csv(\"D://Hiral//Study//PySpark//test2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b4de45-c3fa-4c64-b782-86b5c802c22a",
   "metadata": {},
   "source": [
    "## Create sql table in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c2f0e3-41e5-4fdc-99c7-c0cf3c3c2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "df.createOrReplaceTempView(\"Emp\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae21dff-eb33-4865-9e45-bad852d6c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select column from the table \n",
    "df.select(\"Name\",\"Departments\",\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a435a357-5036-403d-8319-fb62c6407164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql where clause\n",
    "df.select(\"Name\",\"Departments\",\"Salary\").where(\"Departments = 'IOT'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eceea0-fd8d-4eee-93fb-2b3951d48364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sql order by\n",
    "df.select(\"Name\",\"Departments\",\"Salary\").where(\"Departments in ('Data Science','IOT','Big Data')\").orderBy(\"Departments\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637344fc-5784-4335-8e32-e3f7aeece503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all the columns \n",
    "df.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d889d0d5-d758-4580-8ec0-04f47263466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by \n",
    "df.groupby(\"Departments\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a914a45-f457-4610-a90d-00f420bd5f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alias \n",
    "from pyspark.sql.functions import when,col\n",
    "from pyspark.sql import functions as F\n",
    "df.select(F.col(\"Departments\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6614fa0-2cbe-42fc-94c6-ef60d6742c05",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Aggregate Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c0d146-1c85-4644-89dc-d5583286446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first() function returns the first element in a column when ignoreNulls is set to true, it returns the first non-null element.\n",
    "from pyspark.sql.functions import first\n",
    "df.select(first(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da270501-3592-4409-ad53-ce6fd3623a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last() function returns the last element in a column. when ignoreNulls is set to true, it returns the last non-null element.\n",
    "from pyspark.sql.functions import last\n",
    "df.select(last(\"Salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4645fac0-1281-4db9-b7ed-942795d7c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# approx_count_distinct() function returns the count of distinct items in a group.\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "print(\"approx_count_distinct: \" + \\\n",
    "      str(df.select(approx_count_distinct(\"Departments\")).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b3c4e8-ee7b-4f42-88e3-a65dbe2871ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg() function returns the average of values in the input column.\n",
    "from pyspark.sql.functions import avg\n",
    "print(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff6ae03-648d-43b3-a5fb-1fb943d30441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect_set() function returns all values from an input column with duplicate values eliminated.\n",
    "from pyspark.sql.functions import collect_set\n",
    "df.select(collect_set(\"Departments\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f311d90-dcf7-4457-a72e-04c24aa11862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count() function returns number of elements in a column.\n",
    "from pyspark.sql.functions import count\n",
    "print(\"count: \"+str(df.select(count(\"Departments\")).collect()[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226a73b-2118-4f4d-8725-448a33fcc8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kurtosis() function returns the kurtosis of the values in a group.\n",
    "from pyspark.sql.functions import kurtosis\n",
    "df.select(kurtosis(\"Salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3000de30-67ea-4c5d-ac1b-1dee0a96c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max() function returns the maximum value in a column.\n",
    "from pyspark.sql.functions import max\n",
    "df.select(max(\"Salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe03fcd2-ccee-44fc-8e14-aa084af86dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min() function returns the minimum value in a column.\n",
    "from pyspark.sql.functions import min\n",
    "df.select(min(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b72f1f-9352-4bff-b92f-e3821ea39406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean() function returns the average of the values in a column. Alias for Avg\n",
    "from pyspark.sql.functions import mean \n",
    "df.select(mean(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42217831-b86a-4893-8e88-3e1320b46b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewness() function returns the skewness of the values in a group.\n",
    "from pyspark.sql.functions import skewness\n",
    "df.select(skewness(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0833d484-5733-4bf7-8a88-21c428c59f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stddev() alias for stddev_samp.\n",
    "# stddev_samp() function returns the sample standard deviation of values in a column.\n",
    "# stddev_pop() function returns the population standard deviation of the values in a column.\n",
    "\n",
    "from pyspark.sql.functions import stddev,stddev_samp,stddev_pop\n",
    "df.select(stddev(\"Salary\"),stddev_samp(\"Salary\"),stddev_pop(\"Salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a49ef8-856c-419e-8085-96b65153cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum() function Returns the sum of all values in a column.\n",
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e464a80-8556-4ad0-afc0-5d209a62c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumDistinct() function returns the sum of all distinct values in a column.\n",
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d6254-5548-4494-ab15-ae4c4841b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance() alias for var_samp\n",
    "# var_samp() function returns the unbiased variance of the values in a column.\n",
    "# var_pop() function returns the population variance of the values in a column.\n",
    "from pyspark.sql.functions import variance,var_samp,var_pop\n",
    "df.select(variance(\"Salary\"),var_samp(\"Salary\"),var_pop(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f10fde-128a-41b0-874d-ebce4f0b0d2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f8612-5d7d-4214-baf4-c81dc754aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# row_number() window function gives the sequential row number starting from 1 to the result of each window partition.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "windowSpec = Window.partitionBy(\"Departments\").orderBy(\"Salary\")\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5364c4-48e5-4668-b1ca-e32dd683d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank() window function provides a rank to the result within a window partition. This function leaves gaps in rank when there are ties.\n",
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abd1362-4fca-4162-b526-8bd2f5634292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense_rank() window function is used to get the result with rank of rows within a window partition without any gaps.\n",
    "from pyspark.sql.functions import dense_rank \n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491de2ab-1677-45d2-83a1-95b2a27812d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent_rank()\n",
    "from pyspark.sql.functions import percent_rank\n",
    "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da3549-36bf-49b8-a8e7-1525cb155874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ntile() window function returns the relative rank of result rows within a window partition.\n",
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\",ntile(2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc41e7b-3435-4dc7-9a9c-1e9e5416f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cume_dist() This function computes the cumulative distribution of a value within a window partition.\n",
    "from pyspark.sql.functions import cume_dist\n",
    "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5024ba48-6408-42d2-9ee1-29953ea6e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag() function allows you to access a previous row’s value within the partition based on a specified offset. \n",
    "from pyspark.sql.functions import lag\n",
    "df.withColumn(\"lag\",lag(\"Salary\",2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e3669d-307b-4d1d-b980-9a26c34d6789",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lead() function retrieves the column value from the following row within the partition based on a specified offset. \n",
    "from pyspark.sql.functions import lead    \n",
    "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa35e29-fff5-4f62-908e-9e257a40b1cd",
   "metadata": {},
   "source": [
    "## PySpark SQL Date and Timestamp Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84939085-d290-4d8c-bd1f-b5a2b1725e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_date() to get the current system date. By default, the data will be returned in yyyy-dd-mm format.\n",
    "df.select(current_date().alias(\"current_date\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee97cd-bf99-4e6b-94e7-d4e84c25f1e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select(current_timestamp().alias(\"current_timestamp\")\n",
    "  ).show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73edfe3-57be-4330-8cec-f6294fc3180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add default date filed in file \n",
    "df = df.withColumn(\"Date\",current_date())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1443e1-14e8-428f-8d37-d92297420c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses date_format() to parses the date and converts from yyyy-dd-mm to MM-dd-yyyy format.\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"Date\"), \n",
    "    date_format(col(\"Date\"), \"MM-dd-yyyy\").alias(\"date_format\") \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e931a1-5ab0-46aa-aa68-03c629cc7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts string in date format yyyy-MM-dd to a DateType yyyy-MM-dd using to_date(). \n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"Date\"),to_date(col(\"Date\"),\"yyyy-MM-dd\").alias(\"to_date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5146a45-af93-4e49-9506-ba45df929fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the difference between two dates using datediff().\n",
    "df.select(col(\"Date\"),datediff(current_date(),col(\"Date\")).alias(\"datediff\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6f152c-66b5-41ea-a257-b60f6fccc536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  returns the months between two dates using months_between().\n",
    "df.select(col('Date'),months_between(current_date(),col(\"Date\")).alias(\"Months\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b9578-c076-42a3-a80a-e8c78d9cd6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncates the date at a specified unit using trunc().\n",
    "df.select(col(\"Date\"),trunc(col('Date'),\"Month\").alias(\"Month_Trunc\"),\n",
    "          trunc(col(\"Date\"),\"Year\").alias(\"Year_Trunc\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0796cc65-5f67-44c0-8947-e63e1b43bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding and subtracting date and month from a given input.\n",
    "df.select(col(\"Date\"), \n",
    "    add_months(col(\"Date\"),3).alias(\"add_months\"), \n",
    "    add_months(col(\"Date\"),-3).alias(\"sub_months\"), \n",
    "    date_add(col(\"Date\"),4).alias(\"date_add\"), \n",
    "    date_sub(col(\"Date\"),4).alias(\"date_sub\") \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852b5c8d-914f-4c26-b6d5-d8e2bea2e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# year(), month(), month(),next_day(), weekofyear()\n",
    "df.select(col(\"Date\"), \n",
    "     year(col(\"Date\")).alias(\"year\"), \n",
    "     month(col(\"Date\")).alias(\"month\"), \n",
    "     next_day(col(\"Date\"),\"Sunday\").alias(\"next_day\"), \n",
    "     weekofyear(col(\"Date\")).alias(\"weekofyear\") \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1341db57-5314-4366-ba9b-041cf62b7926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dayofweek(), dayofmonth(), dayofyear()\n",
    "df.select(col(\"Date\"),  \n",
    "     dayofweek(col(\"Date\")).alias(\"dayofweek\"), \n",
    "     dayofmonth(col(\"Date\")).alias(\"dayofmonth\"), \n",
    "     dayofyear(col(\"Date\")).alias(\"dayofyear\"), \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53047a-ac44-4634-943b-880522aaa647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add default time filed in file \n",
    "df = df.withColumn(\"Date\",current_timestamp())\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad8f46-f738-4e76-8af9-2c23aafb20a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts string timestamp to Timestamp type format.\n",
    "df.select(col(\"Date\"), \n",
    "    to_timestamp(col(\"Date\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\") \n",
    "  ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4518e08a-1145-48ef-b4c5-9231019bee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hour(), Minute() and second()\n",
    "df.select(col(\"Date\"), \n",
    "    hour(col(\"Date\")).alias(\"hour\"), \n",
    "    minute(col(\"Date\")).alias(\"minute\"),\n",
    "    second(col(\"Date\")).alias(\"second\") \n",
    "  ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668efcfa-d4e8-4432-8bd5-f0065d130348",
   "metadata": {},
   "source": [
    "## PySpark SQL Types (DataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c1eaf-2c2b-4448-8505-ec25df86140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "arrayType = ArrayType(IntegerType(),False)\n",
    "print(arrayType.jsonValue())\n",
    "print(arrayType.simpleString())\n",
    "print(arrayType.typeName())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777723f-7194-490f-bc57-74adce26b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ArrayType to represent arrays in a DataFrame and use ArrayType() to get an array object of a specific type.\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "arrayType = ArrayType(IntegerType(),False)\n",
    "print(arrayType.containsNull)\n",
    "print(arrayType.elementType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2de1d3-fd86-472e-a2d4-bbbf150e55fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MapType to represent key-value pair in a DataFrame. Use MapType() to get a map object of a specific key and value type.\n",
    "from pyspark.sql.types import MapType,StringType,IntegerType\n",
    "mapType = MapType(StringType(),IntegerType())\n",
    " \n",
    "print(mapType.keyType)\n",
    "print(mapType.valueType)\n",
    "print(mapType.valueContainsNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f661a1dc-f320-47a3-a337-33207d930ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9308d75b-13e1-45d6-9260-734fb2804f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1193506c-88c8-414f-a9e2-14f371ce3693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
